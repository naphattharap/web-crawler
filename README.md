# Web-crawler
It is for learning how to retrieve links from the seed URL.
It starts from checking the permission if the website or links allow to be crawled. The robotparser library is used for this task.

Once the website allows for crawling, the links will be retrieved and added to Queue until number of links have reached to defined limit.
